{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNfwugB5OgTrX30y8vrMRJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brainmentorspvtltd/IGDTU_PyTorchTraining/blob/main/IG_ResNet_HAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7pfjdCwAgXy",
        "outputId": "6707ef8e-1bf0-4fef-d47f-d98e2aad4c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading human-action-recognition-har-dataset.zip to /content\n",
            " 99% 294M/297M [00:02<00:00, 160MB/s]\n",
            "100% 297M/297M [00:02<00:00, 140MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"ravikanttyagi\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"095a21c789eb4728fde2b29230033273\"\n",
        "\n",
        "!kaggle datasets download meetnagadia/human-action-recognition-har-dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!unzip human-action-recognition-har-dataset.zip\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "7eMBWKppAmsb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms, models\n",
        "from torch.utils import data\n",
        "import torchvision.datasets as datasets"
      ],
      "metadata": {
        "id": "AmjeufQ-ApXa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "-y1qqVQBAwIY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"Human Action Recognition/train\"\n",
        "test_path = \"Human Action Recognition/test\""
      ],
      "metadata": {
        "id": "kwwTx_DxAyPs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Human Action Recognition/Training_set.csv\")\n",
        "class_names = pd.value_counts(df['label']).index\n",
        "class_names = np.sort(class_names)\n",
        "print(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaybHdxTAzf2",
        "outputId": "2050df53-f627-4618-8655-2ea0003ee062"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['calling' 'clapping' 'cycling' 'dancing' 'drinking' 'eating' 'fighting'\n",
            " 'hugging' 'laughing' 'listening_to_music' 'running' 'sitting' 'sleeping'\n",
            " 'texting' 'using_laptop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = df['filename'].values"
      ],
      "metadata": {
        "id": "PIl1NSrrEmdo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(filenames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2tv9WyDEmZT",
        "outputId": "37097937-f4e7-4d5f-e21b-73358a574615"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12600"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "def load_data(path, df):\n",
        "  images_list = []\n",
        "  labels_list = []\n",
        "  for i in tqdm(range(len(filenames) - 8000)):\n",
        "    # concat train_path with image name\n",
        "    img_path = path + \"/\" + filenames[i]\n",
        "    # fetch image label from data frame of current image\n",
        "    img_label = df['label'][i]\n",
        "    # read image using opencv\n",
        "    img = cv2.imread(img_path)\n",
        "    # resize image because images might be of different dimensions\n",
        "    # in order to maintain array, we have to resize all the images in same dimension\n",
        "    # img = cv2.resize(img, (150,150))\n",
        "    # img = transform(img)\n",
        "    # img = img / 255.0\n",
        "    # store images one by one in your list\n",
        "    images_list.append(img)\n",
        "    labels_list.append(img_label)\n",
        "\n",
        "  images_arr = np.asarray(images_list)\n",
        "  labels_arr = np.asarray(labels_list)\n",
        "\n",
        "  return images_arr, labels_arr"
      ],
      "metadata": {
        "id": "3EMIP6YaA4h-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, train_labels = load_data(train_path, df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBhgWQHBA7Y4",
        "outputId": "8f04159d-0398-4620-8cd3-9c1b92c08ab9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4600/4600 [00:06<00:00, 695.77it/s]\n",
            "<ipython-input-7-ac9a6639ff33>:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  images_arr = np.asarray(images_list)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(train_labels, return_counts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXHRYQxPE8k2",
        "outputId": "b5d5b108-c574-4cd0-b9be-c7ba69148c7d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['calling', 'clapping', 'cycling', 'dancing', 'drinking', 'eating',\n",
              "        'fighting', 'hugging', 'laughing', 'listening_to_music', 'running',\n",
              "        'sitting', 'sleeping', 'texting', 'using_laptop'], dtype='<U18'),\n",
              " array([322, 312, 315, 296, 307, 313, 307, 316, 332, 272, 290, 317, 315,\n",
              "        283, 303]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inherit Dataset class coming from data package\n",
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, images, labels):\n",
        "    self.transforms = transforms\n",
        "    self.images = images\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # loading data - one image at a time\n",
        "    X = self.images[index]\n",
        "    X = cv2.resize(X,(224,224))\n",
        "    y = self.labels[index]\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        transforms.RandomHorizontalFlip()\n",
        "        ])\n",
        "    X = transform(X)\n",
        "    # X = torch.tensor(X)\n",
        "    # X = torch.cat((X,X,X),0)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "hl2FG-K0A94K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = LabelEncoder()\n",
        "train_labels = label.fit_transform(train_labels)"
      ],
      "metadata": {
        "id": "bFmktAqmBHKu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 75% - training and 25% - testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_images, train_labels, test_size=0.25)"
      ],
      "metadata": {
        "id": "L8i2s08IBM_1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7bAYXHkFDhp",
        "outputId": "d9c38d4b-c8e1-4eec-e0dd-dcbdf9fb19cc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3450,)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgqSPVxuFDcD",
        "outputId": "5aa91a32-c00d-4a69-869d-fb38372a70c1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1150,)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\"batch_size\":32, \"shuffle\":True}\n",
        "\n",
        "training_set = Dataset(x_train, y_train)\n",
        "training_generator = data.DataLoader(training_set, **params)\n",
        "\n",
        "test_set = Dataset(x_test, y_test)\n",
        "test_generator = data.DataLoader(test_set, **params)"
      ],
      "metadata": {
        "id": "2C0-dvioBOtv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "# Evaluation metric\n",
        "def accuracy(y_true, y_pred):\n",
        "  # y_true = 1, y_pred = 1\n",
        "  # y_true = 0, y_pred = 0\n",
        "  correct_classification = torch.eq(y_true, y_pred).sum().item()\n",
        "  acc = (correct_classification / len(y_pred)) * 100\n",
        "  return acc\n",
        "\n",
        "def train_step(epoch, model, data, loss_fn, optimizer):\n",
        "  train_loss, train_acc = 0,0\n",
        "  # model.to(device)\n",
        "\n",
        "  for batch, (X, y) in enumerate(data):\n",
        "    X,y = X.to(device), y.to(device)\n",
        "\n",
        "    # Feedforward - it calls forward method inside Model Class\n",
        "    y_pred = model(X)\n",
        "    # Calculate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss\n",
        "    train_acc += accuracy(y, y_pred.argmax(dim=1))\n",
        "\n",
        "    # Backpropagate\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss /= len(data)\n",
        "  train_acc /= len(data)\n",
        "  train_acc_history.append(train_acc)\n",
        "  train_loss_history.append(train_loss)\n",
        "  print(f\"Epoch : {epoch} | Train Loss : {train_loss:.3f} |  Train Acc : {train_acc:.3f}\")\n",
        "\n",
        "\n",
        "def test_step(epoch, model, data, loss_fn, optimizer):\n",
        "  test_loss, test_acc = 0,0\n",
        "  # model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for batch, (X, y) in enumerate(data):\n",
        "      X,y = X.to(device), y.to(device)\n",
        "\n",
        "      # Feedforward - it calls forward method inside Model Class\n",
        "      y_pred = model(X)\n",
        "      # Calculate loss\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      test_loss += loss\n",
        "      test_acc += accuracy(y, y_pred.argmax(dim=1))\n",
        "\n",
        "    test_loss /= len(data)\n",
        "    test_acc /= len(data)\n",
        "    print(f\"Epoch : {epoch} | Test Loss : {test_loss:.3f} |  Test Acc : {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "DaSQHxxhBQZp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_resnet = models.resnet18(weights=\"IMAGENET1K_V1\")"
      ],
      "metadata": {
        "id": "0rHaS-NkBfvx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = model_resnet.fc.in_features"
      ],
      "metadata": {
        "id": "fKFnjBSwCHKq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2Eu7OjWCOHZ",
        "outputId": "b65eeb1e-cea5-4a41-9122-d846a6c343d1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_resnet.fc = nn.Linear(num_features, 15)\n",
        "model_resnet = model_resnet.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_resnet.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "qsZGoMFOCPBm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  train_step(epoch, model_resnet, training_generator, loss_fn, optimizer)\n",
        "  # test_step(epoch, model_resnet, test_generator, loss_fn, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmouU2dfCty_",
        "outputId": "b2106a92-4309-4dfc-ad0f-ca0f7f0734cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:18<02:50, 18.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0 | Train Loss : 1.943 |  Train Acc : 37.317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:29<01:52, 14.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1 | Train Loss : 1.544 |  Train Acc : 50.067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:39<01:25, 12.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 2 | Train Loss : 1.297 |  Train Acc : 57.730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:49<01:08, 11.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 3 | Train Loss : 1.051 |  Train Acc : 65.240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oMFB7J5WC7hb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}